import streamlit as st
import pandas as pd
import numpy as np
from urllib.parse import urlparse
import io
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows
import zipfile
from datetime import datetime
import math
import plotly.express as px
from io import BytesIO

def main():
    st.set_page_config(
        page_title="Audit SÃ©mantique - Gap Content SEO",
        page_icon="ðŸ”",
        layout="wide"
    )
    
    st.title("Outil d'Audit SÃ©mantique - Gap Content SEO")
    st.markdown("---")
    
    # PrÃ©sentation de l'outil
    with st.expander("â„¹ï¸ En quoi consiste ce script ?"):
        st.markdown("""
        Ce script sert Ã  identifier rapidement les opportunitÃ©s de trafic sur base du gap content (contenu non adressÃ©) entre votre domaine et un ou plusieurs domaines concurrents.
        
        **Comment faire ?**
        
        **1. PrÃ©parez vos fichiers**
        * Exportez vos donnÃ©es SEO (Semrush, Ahrefs, etc.) en CSV/Excel,
        * **Important** : TÃ©lÃ©chargez votre site en premier si vous voulez aller plus vite.
        
        **2. Configurez l'analyse**
        * **Source** : Semrush/Ahrefs/Custom,
        * **Concurrents minimum** : 2 (recommandÃ©),
        * **Position max** : Top 10 (page 1),
        * **Filtres** : Volume min selon vos besoins.
        """)
    
    st.markdown("---")
    
    # Sidebar pour la configuration
    with st.sidebar:
        st.header("âš™ï¸ Configuration")
        
        # Section 1: Type de source de donnÃ©es
        st.subheader("1. Source des donnÃ©es")
        data_source = st.selectbox(
            "Type de fichier d'export",
            ["Semrush", "Ahrefs", "Custom"]
        )
        
        # Section 2: Configuration des colonnes (si Custom)
        if data_source == "Custom":
            st.subheader("2. Mapping des colonnes")
            col_keyword = st.text_input("Nom colonne Mot-clÃ©", "Keyword")
            col_domain = st.text_input("Nom colonne Domaine", "Domain")
            col_position = st.text_input("Nom colonne Position", "Position")
            col_volume = st.text_input("Nom colonne Volume de recherche", "Search Volume")
            col_difficulty = st.text_input("Nom colonne DifficultÃ©", "Keyword Difficulty")
            col_intent = st.text_input("Nom colonne Intention", "Keyword Intents")
            col_url = st.text_input("Nom colonne URL", "URL")
        else:
            # Configuration prÃ©dÃ©finie pour Semrush/Ahrefs
            if data_source == "Semrush":
                col_mapping = {
                    'keyword': 'Keyword',
                    'domain': 'URL',  # On extraira le domaine de l'URL
                    'position': 'Position',
                    'volume': 'Search Volume',
                    'difficulty': 'Keyword Difficulty',
                    'intent': 'Keyword Intents',
                    'url': 'URL'
                }
            else:  # Ahrefs
                col_mapping = {
                    'keyword': 'Keyword',
                    'domain': None,  # Pas de colonne URL dans ce format, on utilisera le nom de fichier
                    'position': 'Average position',
                    'volume': 'Volume',
                    'difficulty': None,  # Pas de difficultÃ© dans ce format
                    'intent': None,  # Pas d'intention dans ce format
                    'url': None,  # Pas d'URL spÃ©cifique
                    'traffic': 'Organic traffic'
                }
        
        # Section 3: CritÃ¨res de filtrage
        st.subheader("2. CritÃ¨res Gap Content" if data_source != "Custom" else "3. CritÃ¨res Gap Content")
        min_competitors = st.selectbox(
            "Nombre minimum de concurrents positionnÃ©s",
            [1, 2, 3]
        )
        
        # Position maximum avec curseur et saisie manuelle
        st.write("Position maximum")
        max_position = st.slider(
            "",
            min_value=1,
            max_value=100,
            value=10,
            step=1,
            label_visibility="collapsed"
        )
        
        max_position = st.number_input(
            "Saisissez manuellement",
            min_value=1,
            max_value=100,
            value=max_position,
            step=1
        )
        
        # Filtres supplÃ©mentaires
        st.subheader("3. Filtres supplÃ©mentaires" if data_source != "Custom" else "4. Filtres supplÃ©mentaires")
        min_volume = st.number_input("Volume de recherche minimum", min_value=0, value=0)
        max_difficulty = st.number_input("DifficultÃ© maximum", min_value=0, max_value=100, value=100)
        
        # Filtre termes de marque
        brand_terms = st.text_input(
            "Termes de marque (sÃ©parÃ©s par des virgules)",
            placeholder="marque1, marque2, marque3",
            help="Mots-clÃ©s contenant ces termes seront filtrÃ©s de l'analyse"
        )

    # Zone principale
    st.header("ðŸ“ Import des fichiers")
    
    # Upload des fichiers
    uploaded_files = st.file_uploader(
        "TÃ©lÃ©chargez vos fichiers CSV/Excel (le premier sera considÃ©rÃ© comme votre domaine principal)",
        accept_multiple_files=True,
        type=['csv', 'xlsx', 'xls']
    )
    
    # Identification du domaine principal
    if uploaded_files:
        st.subheader("Identification du domaine principal")
        
        # Option 1: Premier fichier par dÃ©faut
        main_domain_option = st.radio(
            "Comment identifier votre domaine principal ?",
            ["Premier fichier tÃ©lÃ©chargÃ©", "SÃ©lection manuelle"]
        )
        
        if main_domain_option == "SÃ©lection manuelle":
            main_domain = st.text_input("Saisissez votre domaine principal (ex: www.monsite.com)")
        else:
            main_domain = None
    
    # Bouton d'analyse
    if uploaded_files and st.button("Lancer l'analyse", type="primary"):
        with st.spinner("Analyse en cours..."):
            try:
                # Traitement des fichiers
                all_data = process_files(uploaded_files, data_source, locals())
                
                if all_data is not None and not all_data.empty:
                    # Identification du domaine principal
                    if main_domain_option == "Premier fichier tÃ©lÃ©chargÃ©":
                        first_file_data = load_file(uploaded_files[0], data_source, locals())
                        if not first_file_data.empty:
                            main_domain = extract_domain_from_data(first_file_data)
                            st.success(f"Domaine principal dÃ©tectÃ© automatiquement : {main_domain}")
                    
                    # Filtrage des termes de marque
                    if brand_terms.strip():
                        all_data = filter_brand_terms(all_data, brand_terms)
                    
                    # Analyse du gap content
                    gap_analysis = perform_gap_analysis(
                        all_data, 
                        main_domain, 
                        min_competitors, 
                        max_position,
                        min_volume,
                        max_difficulty
                    )
                    
                    # Analyse du domaine principal
                    main_domain_analysis = analyze_main_domain(all_data, main_domain) if main_domain else None
                    
                    if gap_analysis['gap_content'].empty:
                        st.warning("Aucune opportunitÃ© de gap content trouvÃ©e avec ces critÃ¨res.")
                    else:
                        # GÃ©nÃ©ration du fichier Excel
                        excel_file = generate_excel_report(gap_analysis, main_domain, main_domain_analysis)
                        
                        # Affichage d'un rÃ©sumÃ© simple
                        st.success(f"Analyse terminÃ©e ! {len(gap_analysis['gap_content'])} opportunitÃ©s trouvÃ©es.")
                        
                        if not gap_analysis['gap_content'].empty:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Total opportunitÃ©s", len(gap_analysis['gap_content']))
                            with col2:
                                avg_volume = gap_analysis['gap_content']['volume'].mean()
                                st.metric("Volume moyen", f"{avg_volume:,.0f}")
                            with col3:
                                total_volume = gap_analysis['gap_content']['volume'].sum()
                                st.metric("Volume total", f"{total_volume:,.0f}")
                        
                        # Bouton de tÃ©lÃ©chargement
                        st.download_button(
                            label="TÃ©lÃ©charger le rapport Excel",
                            data=excel_file,
                            file_name=f"gap_content_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                else:
                    st.error("Impossible de traiter les fichiers tÃ©lÃ©chargÃ©s.")
                    
            except Exception as e:
                st.error(f"Erreur lors de l'analyse : {str(e)}")


def extract_domain(url):
    """Extrait le domaine racine d'une URL (sans sous-domaines)"""
    try:
        if pd.isna(url) or url == '':
            return ''
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # Extraire le domaine racine (enlever les sous-domaines)
        domain_parts = domain.split('.')
        if len(domain_parts) >= 2:
            # Garder seulement les deux derniÃ¨res parties (domaine.tld)
            # GÃ©rer les cas spÃ©ciaux comme .co.uk, .com.fr, etc.
            if len(domain_parts) >= 3 and domain_parts[-2] in ['co', 'com', 'net', 'org', 'gov']:
                root_domain = '.'.join(domain_parts[-3:])
            else:
                root_domain = '.'.join(domain_parts[-2:])
            return root_domain
        return domain
    except:
        return ''


def extract_domain_from_data(df):
    """Extrait le domaine racine principal du premier fichier"""
    if 'domain' in df.columns and not df['domain'].empty:
        # Prendre le domaine le plus frÃ©quent
        return df['domain'].value_counts().index[0]
    return None


def load_file(file, data_source, config):
    """Charge un fichier CSV ou Excel et normalise les colonnes"""
    try:
        if file.name.endswith('.csv'):
            # DÃ©tection automatique de l'encodage pour les CSV
            df = None
            encodings_to_try = ['utf-8', 'utf-16', 'utf-8-sig', 'latin-1', 'cp1252', 'utf-16-le', 'utf-16-be']
            separators_to_try = [',', '\t', ';']
            
            for encoding in encodings_to_try:
                for separator in separators_to_try:
                    try:
                        # Reset du pointeur de fichier
                        file.seek(0)
                        df = pd.read_csv(file, encoding=encoding, sep=separator)
                        
                        # VÃ©rifier si le DataFrame a du sens (plus d'une colonne)
                        if len(df.columns) > 1:
                            st.success(f"Fichier {file.name} chargÃ© avec l'encodage {encoding} et sÃ©parateur '{separator}'")
                            break
                    except (UnicodeDecodeError, UnicodeError, pd.errors.ParserError):
                        continue
                    except Exception as e:
                        continue
                if df is not None and len(df.columns) > 1:
                    break
            
            if df is None or len(df.columns) <= 1:
                # Tentative avec dÃ©tection automatique Python
                try:
                    file.seek(0)
                    # Essayer avec engine python qui est plus permissif
                    df = pd.read_csv(file, encoding='utf-8', sep=None, engine='python', error_bad_lines=False)
                    st.warning(f"Fichier {file.name} chargÃ© avec dÃ©tection automatique")
                except:
                    raise ValueError(f"Impossible de dÃ©tecter l'encodage du fichier {file.name}")
        else:
            # Pour les fichiers Excel, pandas gÃ¨re automatiquement l'encodage
            df = pd.read_excel(file)
        
        # Debug : afficher les colonnes disponibles
        st.info(f"Colonnes dÃ©tectÃ©es dans {file.name}: {list(df.columns)}")
        
        # Normalisation des colonnes selon la source
        if data_source == "Semrush":
            if 'Keyword' in df.columns:
                df['domain'] = df['URL'].apply(extract_domain)
                df = df.rename(columns={
                    'Keyword': 'keyword',
                    'Position': 'position',
                    'Search Volume': 'volume',
                    'Keyword Difficulty': 'difficulty',
                    'Keyword Intents': 'intent',
                    'URL': 'url'
                })
        elif data_source == "Ahrefs":
            # Extraction du domaine depuis le nom du fichier pour Ahrefs
            filename = file.name
            domain_from_filename = 'unknown.com'
            
            # Extraction amÃ©liorÃ©e du domaine depuis le nom de fichier
            if '-organic' in filename:
                domain_part = filename.split('-organic')[0]  # Prendre la partie avant "-organic"
                
                # Nettoyer le domaine extrait
                if domain_part.startswith('www.'):
                    domain_part = domain_part[4:]  # Enlever "www."
                
                # GÃ©rer les cas spÃ©ciaux comme "invivo-group.com-fr"
                if domain_part.endswith('-fr'):
                    domain_part = domain_part[:-3]  # Enlever "-fr"
                elif domain_part.endswith('.com-fr'):
                    domain_part = domain_part.replace('.com-fr', '.com')
                elif domain_part.endswith('.fr-fr'):
                    domain_part = domain_part.replace('.fr-fr', '.fr')
                
                domain_from_filename = domain_part
            elif '.' in filename:
                # Fallback : prendre la partie avant le premier tiret ou espace
                domain_part = filename.split('-')[0].split('_')[0].split(' ')[0]
                if domain_part.startswith('www.'):
                    domain_part = domain_part[4:]
                domain_from_filename = domain_part
            
            # S'assurer que le domaine a une extension valide
            if not ('.' in domain_from_filename and any(ext in domain_from_filename for ext in ['.com', '.fr', '.org', '.net', '.coop'])):
                domain_from_filename = domain_from_filename + '.com'
            
            df['domain'] = extract_domain(f"https://{domain_from_filename}")
            df['url'] = f"https://{domain_from_filename}"  # URL gÃ©nÃ©rique
            
            # Debug : afficher le domaine extrait
            st.info(f"Domaine extrait pour {filename}: {domain_from_filename} â†’ {df['domain'].iloc[0] if len(df) > 0 else 'N/A'}")
            
            # Mapping flexible des colonnes Ahrefs (CSV vs Excel peuvent diffÃ©rer)
            column_mapping = {}
            
            # Chercher les colonnes par nom (insensible Ã  la casse)
            for col in df.columns:
                col_lower = col.lower().strip()
                if 'keyword' in col_lower:
                    column_mapping['keyword'] = col
                elif 'volume' in col_lower and 'traffic' not in col_lower and 'location' not in col_lower:
                    column_mapping['volume'] = col
                elif ('position' in col_lower or 'rank' in col_lower) and 'average' in col_lower:
                    column_mapping['position'] = col
                elif 'traffic' in col_lower and 'organic' in col_lower:
                    column_mapping['traffic'] = col
                elif 'difficulty' in col_lower or 'kd' in col_lower:
                    column_mapping['difficulty'] = col
            
            # Debug : afficher le mapping trouvÃ©
            st.info(f"Mapping des colonnes pour {filename}: {column_mapping}")
            
            # Renommer les colonnes trouvÃ©es
            if column_mapping:
                df = df.rename(columns=column_mapping)
            
            # S'assurer que les colonnes essentielles existent
            if 'keyword' not in df.columns:
                # Essayer de trouver une colonne qui pourrait Ãªtre le mot-clÃ©
                possible_keyword_cols = [col for col in df.columns if any(term in col.lower() for term in ['keyword', 'query', 'term'])]
                if possible_keyword_cols:
                    df = df.rename(columns={possible_keyword_cols[0]: 'keyword'})
                else:
                    raise ValueError(f"Impossible de trouver une colonne 'keyword' dans {file.name}")
            
            # VÃ©rifier si la colonne volume existe et est correcte
            if 'volume' not in df.columns:
                # Chercher d'autres colonnes possibles pour le volume
                possible_volume_cols = [col for col in df.columns if 'volume' in col.lower() and 'location' not in col.lower()]
                if possible_volume_cols:
                    df = df.rename(columns={possible_volume_cols[0]: 'volume'})
                    st.info(f"Colonne volume trouvÃ©e: {possible_volume_cols[0]} â†’ volume")
                else:
                    df['volume'] = 0  # Valeur par dÃ©faut
                    st.warning(f"Aucune colonne volume trouvÃ©e dans {filename}, utilisation de 0 par dÃ©faut")
            
            # Ajouter des colonnes manquantes avec des valeurs par dÃ©faut
            if 'difficulty' not in df.columns:
                df['difficulty'] = 50  # Valeur par dÃ©faut
            if 'intent' not in df.columns:
                df['intent'] = 'unknown'  # Valeur par dÃ©faut
            if 'position' not in df.columns:
                df['position'] = 50  # Valeur par dÃ©faut
                st.warning(f"Aucune colonne position trouvÃ©e dans {filename}, utilisation de 50 par dÃ©faut")
                
        else:  # Custom
            df['domain'] = df[config.get('col_domain', 'URL')].apply(extract_domain)
            df = df.rename(columns={
                config.get('col_keyword', 'Keyword'): 'keyword',
                config.get('col_position', 'Position'): 'position',
                config.get('col_volume', 'Search Volume'): 'volume',
                config.get('col_difficulty', 'Keyword Difficulty'): 'difficulty',
                config.get('col_intent', 'Keyword Intents'): 'intent',
                config.get('col_url', 'URL'): 'url'
            })
        
        # VÃ©rification finale des colonnes essentielles
        required_columns = ['keyword', 'domain']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Colonnes manquantes aprÃ¨s traitement: {missing_columns}")
        
        # Nettoyage des donnÃ©es
        initial_count = len(df)
        df = df.dropna(subset=['keyword', 'domain'])
        
        # Conversion numÃ©rique avec debug
        if 'position' in df.columns:
            df['position'] = pd.to_numeric(df['position'], errors='coerce')
            st.info(f"Positions dans {file.name}: min={df['position'].min()}, max={df['position'].max()}, sample={df['position'].head(3).tolist()}")
        
        if 'volume' in df.columns:
            df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
            df['volume'] = df['volume'].fillna(0)  # Remplacer NaN par 0
            st.info(f"Volumes dans {file.name}: min={df['volume'].min()}, max={df['volume'].max()}, sample={df['volume'].head(3).tolist()}")
        
        if 'difficulty' in df.columns:
            df['difficulty'] = pd.to_numeric(df['difficulty'], errors='coerce')
        
        # Filtrer les lignes avec des positions invalides
        if 'position' in df.columns:
            df = df[df['position'] > 0]
        
        # Ajout du nom du fichier
        df['source_file'] = file.name
        
        final_count = len(df)
        st.success(f"Fichier {file.name} traitÃ© avec succÃ¨s : {final_count}/{initial_count} lignes valides")
        
        return df
        
    except Exception as e:
        st.error(f"Erreur lors du chargement de {file.name}: {str(e)}")
        return pd.DataFrame()


def process_files(files, data_source, config):
    """Traite tous les fichiers uploadÃ©s"""
    all_dataframes = []
    
    for file in files:
        df = load_file(file, data_source, config)
        if not df.empty:
            all_dataframes.append(df)
    
    if all_dataframes:
        return pd.concat(all_dataframes, ignore_index=True)
    else:
        return pd.DataFrame()


def filter_brand_terms(data, brand_terms):
    """Filtre les mots-clÃ©s contenant des termes de marque"""
    if not brand_terms.strip():
        return data
    
    # Nettoyer et sÃ©parer les termes
    terms = [term.strip().lower() for term in brand_terms.split(',') if term.strip()]
    
    if not terms:
        return data
    
    # Filtrer les mots-clÃ©s contenant ces termes
    def contains_brand_term(keyword):
        if pd.isna(keyword):
            return False
        keyword_lower = str(keyword).lower()
        return any(term in keyword_lower for term in terms)
    
    # Garder seulement les mots-clÃ©s qui ne contiennent pas de termes de marque
    filtered_data = data[~data['keyword'].apply(contains_brand_term)]
    
    return filtered_data


def analyze_main_domain(data, main_domain):
    """Analyse le positionnement du domaine principal"""
    if not main_domain:
        return None
    
    # Filtrer les donnÃ©es du domaine principal
    main_data = data[data['domain'] == main_domain].copy()
    
    if main_data.empty:
        return None
    
    # CatÃ©goriser par position
    categories = {
        'Sauvegarde': main_data[(main_data['position'] >= 1) & (main_data['position'] <= 3)],
        'Quick Win': main_data[(main_data['position'] >= 4) & (main_data['position'] <= 5)],
        'OpportunitÃ©': main_data[(main_data['position'] >= 6) & (main_data['position'] <= 10)],
        'Potentiel': main_data[(main_data['position'] >= 11) & (main_data['position'] <= 20)],
        'ConquÃªte': main_data[(main_data['position'] >= 21) & (main_data['position'] <= 100)]
    }
    
    # Analyser les mots-clÃ©s non positionnÃ©s (prÃ©sents chez les concurrents mais pas chez nous)
    all_keywords = set(data['keyword'].unique())
    main_keywords = set(main_data['keyword'].unique())
    non_positioned = all_keywords - main_keywords
    
    # RÃ©cupÃ©rer les donnÃ©es des mots-clÃ©s non positionnÃ©s
    non_positioned_data = data[data['keyword'].isin(non_positioned)].drop_duplicates('keyword')
    
    return {
        'categories': categories,
        'non_positioned': non_positioned_data,
        'main_domain': main_domain
    }


def perform_gap_analysis(data, main_domain, min_competitors, max_position, min_volume, max_difficulty):
    """Effectue l'analyse du gap content - VERSION CORRIGÃ‰E"""
    
    # Filtrage des positions valides
    data = data[(data['position'] <= max_position) & (data['position'] > 0)]
    
    # Filtrage par volume et difficultÃ©
    if min_volume > 0:
        data = data[data['volume'] >= min_volume]
    if max_difficulty < 100:
        data = data[data['difficulty'] <= max_difficulty]
    
    # Groupement par mot-clÃ©
    keyword_analysis = []
    
    for keyword, group in data.groupby('keyword'):
        # Informations du mot-clÃ©
        volume = group['volume'].iloc[0] if not group['volume'].isna().all() else 0
        difficulty = group['difficulty'].iloc[0] if not group['difficulty'].isna().all() else 0
        intent = group['intent'].iloc[0] if 'intent' in group.columns else ''
        
        # Analyse des domaines positionnÃ©s
        positioned_domains = group[group['position'] <= max_position]
        unique_domains = positioned_domains['domain'].unique()
        
        # VÃ©rifier si le domaine principal est prÃ©sent
        main_domain_present = main_domain in unique_domains if main_domain else False
        competitor_count = len(unique_domains) - (1 if main_domain_present else 0)
        
        # CritÃ¨re de gap content : assez de concurrents positionnÃ©s MAIS domaine principal absent
        if competitor_count >= min_competitors and not main_domain_present:
            
            # Trouver la meilleure position globale et l'URL correspondante (IMPORTANT - pas un doublon !)
            global_best_position = positioned_domains['position'].min()
            global_best_url = positioned_domains[positioned_domains['position'] == global_best_position]['url'].iloc[0]
            
            keyword_data = {
                'keyword': keyword,
                'volume': volume,
                'difficulty': difficulty,
                'intent': intent,
                'competitor_count': competitor_count,
                'best_position': global_best_position,  # Meilleure position parmi TOUS les concurrents
                'best_url': global_best_url,  # URL de cette meilleure position
                'total_domains': len(unique_domains)
            }
            
            # CORRECTION 2: Ajouter une colonne pour le domaine principal mÃªme s'il n'est pas positionnÃ©
            if main_domain:
                main_domain_data = positioned_domains[positioned_domains['domain'] == main_domain]
                if not main_domain_data.empty:
                    # Le domaine principal est positionnÃ© (ne devrait pas arriver dans gap content mais sÃ©curitÃ©)
                    main_domain_position = main_domain_data['position'].min()
                    main_domain_url = main_domain_data[main_domain_data['position'] == main_domain_position]['url'].iloc[0]
                else:
                    # Le domaine principal n'est pas positionnÃ© (cas normal pour gap content)
                    main_domain_position = None
                    main_domain_url = None
                
                # Ajouter les colonnes du domaine principal
                keyword_data[f'{main_domain}_position'] = main_domain_position
                keyword_data[f'{main_domain}_url'] = main_domain_url
            
            # Ajouter les positions et URLs de chaque domaine concurrent
            for domain in unique_domains:
                if domain != main_domain:  # Ã‰viter les doublons avec le domaine principal
                    domain_data = positioned_domains[positioned_domains['domain'] == domain]
                    domain_best_position = domain_data['position'].min()
                    domain_best_url = domain_data[domain_data['position'] == domain_best_position]['url'].iloc[0]
                    
                    keyword_data[f'{domain}_position'] = domain_best_position
                    keyword_data[f'{domain}_url'] = domain_best_url
            
            keyword_analysis.append(keyword_data)
    
    gap_content_df = pd.DataFrame(keyword_analysis)
    
    # CrÃ©ation du rapport par domaine
    domain_reports = {}
    for domain in data['domain'].unique():
        if domain and domain != main_domain:
            domain_data = data[data['domain'] == domain].copy()
            domain_data = domain_data.sort_values(['position', 'volume'], ascending=[True, False])
            domain_reports[domain] = domain_data[['keyword', 'volume', 'position', 'url']].reset_index(drop=True)
    
    return {
        'gap_content': gap_content_df,
        'domain_reports': domain_reports,
        'main_domain': main_domain,
        'all_data': data
    }


def generate_excel_report(analysis, main_domain, main_domain_analysis=None):
    """GÃ©nÃ¨re le rapport Excel avec mise en forme - VERSION CORRIGÃ‰E"""
    
    output = io.BytesIO()
    workbook = Workbook()
    
    # Suppression de la feuille par dÃ©faut
    workbook.remove(workbook.active)
    
    # Styles
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    header_font = Font(color="FFFFFF", bold=True)
    main_domain_fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")  # Vert clair pour domaine principal
    center_alignment = Alignment(horizontal="center", vertical="center")
    
    # 1. Onglet Gap Content Analysis - VERSION CORRIGÃ‰E
    if not analysis['gap_content'].empty:
        ws_gap = workbook.create_sheet("Gap Content Analysis")
        gap_df = analysis['gap_content'].copy()
        
        # CORRECTION 3: RÃ©organiser les colonnes en mettant le domaine principal EN PREMIER
        base_cols = ['keyword', 'volume', 'difficulty', 'intent', 'competitor_count', 'best_position', 'best_url']
        
        # Identifier tous les domaines
        position_columns = [col for col in gap_df.columns if col.endswith('_position')]
        url_columns = [col for col in gap_df.columns if col.endswith('_url')]
        
        # Extraire les noms de domaines
        all_domains = list(set([col.replace('_position', '').replace('_url', '') for col in position_columns + url_columns]))
        
        # CORRECTION: Mettre le domaine principal en premier dans l'ordre
        ordered_domains = []
        if main_domain and main_domain in all_domains:
            ordered_domains.append(main_domain)
        
        # Ajouter les autres domaines triÃ©s
        for domain in sorted(all_domains):
            if domain != main_domain and domain not in ordered_domains:
                ordered_domains.append(domain)
        
        # Construire l'ordre des colonnes : base + domaine principal + autres domaines (positions puis URLs)
        ordered_position_cols = []
        ordered_url_cols = []
        
        for domain in ordered_domains:
            if f'{domain}_position' in gap_df.columns:
                ordered_position_cols.append(f'{domain}_position')
            if f'{domain}_url' in gap_df.columns:
                ordered_url_cols.append(f'{domain}_url')
        
        all_cols = base_cols + ordered_position_cols + ordered_url_cols
        gap_df_display = gap_df[all_cols].copy()
        
        # Renommer les colonnes pour l'affichage
        column_mapping = {
            'keyword': 'Mot-clÃ©',
            'volume': 'Volume de recherche',
            'difficulty': 'DifficultÃ© concurrentielle',
            'intent': 'Intention de recherche',
            'competitor_count': 'Nombre de concurrents positionnÃ©s',
            'best_position': 'Meilleure position globale',
            'best_url': 'URL de la meilleure position'
        }
        
        # Ajouter les noms de domaines dans le mapping
        for domain in ordered_domains:
            domain_name = domain.replace('_', '.')
            if f'{domain}_position' in gap_df.columns:
                if domain == main_domain:
                    column_mapping[f'{domain}_position'] = f'ðŸ  {domain_name} (Position)'
                else:
                    column_mapping[f'{domain}_position'] = f'{domain_name} (Position)'
            if f'{domain}_url' in gap_df.columns:
                if domain == main_domain:
                    column_mapping[f'{domain}_url'] = f'ðŸ  {domain_name} (URL)'
                else:
                    column_mapping[f'{domain}_url'] = f'{domain_name} (URL)'
        
        gap_df_display = gap_df_display.rename(columns=column_mapping)
        
        # Ã‰criture des donnÃ©es
        for r in dataframe_to_rows(gap_df_display, index=False, header=True):
            ws_gap.append(r)
        
        # Mise en forme des en-tÃªtes
        for col_idx, cell in enumerate(ws_gap[1], 1):
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center_alignment
            
            # CORRECTION 2: Mise en forme spÃ©ciale pour les colonnes du domaine principal
            if main_domain and (f'ðŸ  {main_domain}' in str(cell.value)):
                cell.fill = main_domain_fill
                cell.font = Font(color="000000", bold=True)  # Texte noir sur fond vert
        
        # Mise en forme des donnÃ©es - colonnes du domaine principal en vert clair
        if main_domain:
            main_domain_col_indices = []
            for col_idx, cell in enumerate(ws_gap[1], 1):
                if f'ðŸ  {main_domain}' in str(cell.value):
                    main_domain_col_indices.append(col_idx)
            
            # Appliquer le fond vert clair aux donnÃ©es du domaine principal
            for row_idx in range(2, ws_gap.max_row + 1):
                for col_idx in main_domain_col_indices:
                    cell = ws_gap.cell(row=row_idx, column=col_idx)
                    cell.fill = PatternFill(start_color="F0FFF0", end_color="F0FFF0", fill_type="solid")  # Vert trÃ¨s clair
        
        # Ajustement des largeurs de colonnes
        for col_num in range(1, len(gap_df_display.columns) + 1):
            column_letter = ws_gap.cell(row=1, column=col_num).column_letter
            max_length = 0
            for row_num in range(1, ws_gap.max_row + 1):
                cell = ws_gap.cell(row=row_num, column=col_num)
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws_gap.column_dimensions[column_letter].width = adjusted_width
    
    # 2. Onglet rÃ©capitulatif du domaine principal
    if main_domain_analysis:
        ws_main = workbook.create_sheet(f"Analyse {main_domain}")
        
        # Titre principal
        ws_main.append([f"Analyse de positionnement - {main_domain}"])
        ws_main['A1'].font = Font(size=18, bold=True)
        ws_main.merge_cells('A1:D1')
        ws_main.append([])
        
        current_row = 3
        
        # Pour chaque catÃ©gorie
        categories_order = ['Sauvegarde', 'Quick Win', 'OpportunitÃ©', 'Potentiel', 'ConquÃªte']
        position_ranges = {
            'Sauvegarde': '1-3',
            'Quick Win': '4-5', 
            'OpportunitÃ©': '6-10',
            'Potentiel': '11-20',
            'ConquÃªte': '21-100'
        }
        
        for category in categories_order:
            cat_data = main_domain_analysis['categories'][category]
            
            if not cat_data.empty:
                # Titre de catÃ©gorie
                ws_main.append([f"{category} (Positions {position_ranges[category]}) - {len(cat_data)} mots-clÃ©s"])
                ws_main[f'A{current_row}'].font = Font(size=14, bold=True)
                current_row += 1
                
                # En-tÃªtes
                headers = ['Mot-clÃ©', 'Volume de recherche', 'Position', 'URL']
                ws_main.append(headers)
                for cell in ws_main[current_row]:
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = center_alignment
                current_row += 1
                
                # DonnÃ©es triÃ©es par position puis volume
                sorted_data = cat_data.sort_values(['position', 'volume'], ascending=[True, False])
                for _, row in sorted_data.iterrows():
                    ws_main.append([row['keyword'], row['volume'], row['position'], row['url']])
                    current_row += 1
                
                # Mise en forme conditionnelle des volumes
                if len(sorted_data) > 0:
                    min_volume = sorted_data['volume'].min()
                    max_volume = sorted_data['volume'].max()
                    
                    start_row = current_row - len(sorted_data)
                    for row_num in range(start_row, current_row):
                        cell = ws_main[f'B{row_num}']
                        volume = cell.value
                        if volume and max_volume > min_volume:
                            intensity = (volume - min_volume) / (max_volume - min_volume)
                            green_value = int(255 - (intensity * 100))
                            color = f"FF{green_value:02X}FF{green_value:02X}"
                            cell.fill = PatternFill(start_color=color, end_color=color, fill_type="solid")
                
                ws_main.append([])  # Ligne vide
                current_row += 1
        
        # Section Non positionnÃ©
        non_pos_data = main_domain_analysis['non_positioned']
        if not non_pos_data.empty:
            ws_main.append([f"Non positionnÃ© - {len(non_pos_data)} mots-clÃ©s"])
            ws_main[f'A{current_row}'].font = Font(size=14, bold=True)
            current_row += 1
            
            # En-tÃªtes
            headers = ['Mot-clÃ©', 'Volume de recherche', 'DifficultÃ©', 'Intention']
            ws_main.append(headers)
            for cell in ws_main[current_row]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = center_alignment
            current_row += 1
            
            # DonnÃ©es triÃ©es par volume
            sorted_data = non_pos_data.sort_values('volume', ascending=False)
            for _, row in sorted_data.iterrows():
                ws_main.append([row['keyword'], row['volume'], row.get('difficulty', ''), row.get('intent', '')])
                current_row += 1
        
        # Ajustement des largeurs
        for col_num in range(1, 5):
            column_letter = ws_main.cell(row=3, column=col_num).column_letter
            max_length = 0
            for row_num in range(1, ws_main.max_row + 1):
                cell = ws_main.cell(row=row_num, column=col_num)
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws_main.column_dimensions[column_letter].width = adjusted_width
    
    # 3. Onglets pour chaque domaine concurrent
    for domain, domain_data in analysis['domain_reports'].items():
        if not domain_data.empty:
            # CrÃ©er un nom d'onglet valide (max 31 caractÃ¨res)
            sheet_name = domain.replace('www.', '').replace('.com', '').replace('.fr', '')[:31]
            ws_domain = workbook.create_sheet(sheet_name)
            
            # Ligne de titre avec le nom du domaine
            ws_domain.append([domain])
            ws_domain['A1'].font = Font(size=18, bold=True)
            ws_domain.merge_cells('A1:D1')
            
            # Ligne vide
            ws_domain.append([])
            
            # En-tÃªtes
            headers = ['Mot-clÃ©', 'Volume de recherche', 'Position', 'URL']
            ws_domain.append(headers)
            
            # Mise en forme des en-tÃªtes
            for cell in ws_domain[3]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = center_alignment
            
            # DonnÃ©es triÃ©es par position puis volume
            sorted_data = domain_data.sort_values(['position', 'volume'], ascending=[True, False])
            
            # Ã‰criture des donnÃ©es
            for _, row in sorted_data.iterrows():
                ws_domain.append([row['keyword'], row['volume'], row['position'], row['url']])
            
            # Mise en forme conditionnelle pour les volumes
            if len(sorted_data) > 0:
                min_volume = sorted_data['volume'].min()
                max_volume = sorted_data['volume'].max()
                
                # Coloration conditionnelle des volumes (colonne B)
                for row_num in range(4, len(sorted_data) + 4):
                    cell = ws_domain[f'B{row_num}']
                    volume = cell.value
                    if volume and max_volume > min_volume:
                        # Gradient du blanc au vert foncÃ©
                        intensity = (volume - min_volume) / (max_volume - min_volume)
                        green_value = int(255 - (intensity * 100))  # De 255 (blanc) Ã  155 (vert clair)
                        color = f"FF{green_value:02X}FF{green_value:02X}"
                        cell.fill = PatternFill(start_color=color, end_color=color, fill_type="solid")
            
            # Ajustement des largeurs
            for col_num in range(1, 5):  # 4 colonnes : Mot-clÃ©, Volume, Position, URL
                column_letter = ws_domain.cell(row=3, column=col_num).column_letter
                max_length = 0
                for row_num in range(1, ws_domain.max_row + 1):
                    cell = ws_domain.cell(row=row_num, column=col_num)
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_domain.column_dimensions[column_letter].width = adjusted_width
    
    # Sauvegarde
    workbook.save(output)
    output.seek(0)
    
    return output.getvalue()


if __name__ == "__main__":
    main()
